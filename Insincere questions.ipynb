{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['embeddings', 'train.csv', 'sample_submission.csv', 'test.csv']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv(\"../input/train.csv\")\nX_train = train_df[\"question_text\"].fillna(\"zxc\").values\ntest_df = pd.read_csv(\"../input/test.csv\")\nX_test = test_df[\"question_text\"].fillna(\"zxc\").values\ny = train_df[\"target\"]\n\ntrain_df.head()",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "                    qid  ...   target\n0  00002165364db923c7e6  ...        0\n1  000032939017120e6e44  ...        0\n2  0000412ca6e4628ce2cf  ...        0\n3  000042bf85aa498cd78e  ...        0\n4  0000455dfa3e01eae3af  ...        0\n\n[5 rows x 3 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question_text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00002165364db923c7e6</td>\n      <td>How did Quebec nationalists see their province...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000032939017120e6e44</td>\n      <td>Do you have an adopted dog, how would you enco...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0000412ca6e4628ce2cf</td>\n      <td>Why does velocity affect time? Does velocity a...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>000042bf85aa498cd78e</td>\n      <td>How did Otto von Guericke used the Magdeburg h...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0000455dfa3e01eae3af</td>\n      <td>Can I convert montra helicon D to a mountain b...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c44c7dcccd256a5c88d793d9fa9599b3016072e8"
      },
      "cell_type": "code",
      "source": "# Wow, people can be awful :(\ntrain_df[train_df.target == 1].head()",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "                      qid  ...   target\n22   0000e91571b60c2fb487  ...        1\n30   00013ceca3f624b09f42  ...        1\n110  0004a7fcb2bf73076489  ...        1\n114  00052793eaa287aff1e1  ...        1\n115  000537213b01fd77b58a  ...        1\n\n[5 rows x 3 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>qid</th>\n      <th>question_text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>22</th>\n      <td>0000e91571b60c2fb487</td>\n      <td>Has the United States become the largest dicta...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>00013ceca3f624b09f42</td>\n      <td>Which babies are more sweeter to their parents...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>110</th>\n      <td>0004a7fcb2bf73076489</td>\n      <td>If blacks support school choice and mandatory ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>114</th>\n      <td>00052793eaa287aff1e1</td>\n      <td>I am gay boy and I love my cousin (boy). He is...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>115</th>\n      <td>000537213b01fd77b58a</td>\n      <td>Which races have the smallest penis?</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e316583cd85b99c4aa277f5e560a8ef9eed630ea"
      },
      "cell_type": "code",
      "source": "# Naive Bayes\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n\nkf = StratifiedKFold(n_splits=3)\n\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('classifier', MultinomialNB()),\n])\n\ncross_val_score(pipeline, X_train, y, cv=kf, scoring='accuracy').mean()",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "0.9407007920400714"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6f0fae8e57fca2bd3e90990d469257ca8327e2c"
      },
      "cell_type": "code",
      "source": "# Naive Bayes\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n\nkf = StratifiedKFold(n_splits=3)\n\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('classifier', MultinomialNB()),\n])\n\nparam_grid = {\n    'tfidf__min_df': [20,],\n    'tfidf__max_df': [.4,],\n    'tfidf__ngram_range': [(1,2)],\n}\n\ngrid = GridSearchCV(pipeline, param_grid, cv=kf, scoring='accuracy').fit(X_train, y)\nclf = grid.best_estimator_\nprint(clf)\nprint()\nprint(cross_val_score(clf, X_train, y, cv=kf, scoring='accuracy').mean())\n\n# 0.949718325755045",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Pipeline(memory=None,\n     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=0.4, max_features=None, min_df=20,\n        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n...      vocabulary=None)), ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])\n\n0.949718325755045\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38a308e5661046f97af541061420bb29f0d5eadb"
      },
      "cell_type": "code",
      "source": "from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, concatenate\nfrom keras.layers import (CuDNNGRU, CuDNNLSTM, Bidirectional, GlobalAveragePooling1D,\n                          GlobalMaxPooling1D)\nfrom keras.preprocessing import text, sequence",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2fe4020ebc3ddf16a3c2f1c53eb67d376f25561"
      },
      "cell_type": "code",
      "source": "maxlen = 100\nmax_features = 30000\n\n# Tokenize\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\n# Sequence\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "805f9708fd42f212b238d254c6958b63b0bd4bb8"
      },
      "cell_type": "code",
      "source": "tokenizer.num_words",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 25,
          "data": {
            "text/plain": "30000"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d702fb1a3ae2b88745f178a2753c60f43ecd32ce"
      },
      "cell_type": "code",
      "source": "def get_model():\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, 100)(inp)\n    x = CuDNNLSTM(64, return_sequences=True)(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    outp = Dense(1, activation=\"sigmoid\")(conc)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel = get_model()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bce1f48157e1d9dad592dfbf38cca02a811adfdf"
      },
      "cell_type": "code",
      "source": "def get_model():\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, 100)(inp)\n    x = CuDNNLSTM(64)(x)\n    x = Dense(32, activation=\"relu\")(x)\n    outp = Dense(1, activation=\"sigmoid\")(x)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel = get_model()",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fec390becf54d8d33f71ca13d613696980a0776e"
      },
      "cell_type": "code",
      "source": "batch_size = 128\nepochs = 10",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6dc865296d4f0254fc04aefe07176d7917f3397"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y, test_size = 0.1, random_state=42)\n\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nes = EarlyStopping(monitor='val_acc', min_delta=.003, patience=2)\nlr = ReduceLROnPlateau(monitor='val_acc', min_delta=.003, patience=1)",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b112231e50369d9e45a687d8e985be999adcb04"
      },
      "cell_type": "code",
      "source": "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n                  verbose=True, callbacks=[es, lr])\n\n\ny_pred = model.predict(x_test, batch_size=1024)",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Train on 1175509 samples, validate on 130613 samples\nEpoch 1/10\n1175509/1175509 [==============================] - 205s 174us/step - loss: 0.1191 - acc: 0.9536 - val_loss: 0.1058 - val_acc: 0.9574\nEpoch 2/10\n1175509/1175509 [==============================] - 203s 172us/step - loss: 0.1008 - acc: 0.9595 - val_loss: 0.1045 - val_acc: 0.9590\nEpoch 3/10\n1175509/1175509 [==============================] - 202s 172us/step - loss: 0.0846 - acc: 0.9665 - val_loss: 0.1097 - val_acc: 0.9574\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7cdc05e409d38dbef23e0f40786147b8b40fb3eb"
      },
      "cell_type": "code",
      "source": "# 50000 loss: 0.1162 - acc: 0.9545 - val_loss: 0.1052 - val_acc: 0.9582",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f1bcd395c3b5831252361217c742eed7cf40e02"
      },
      "cell_type": "code",
      "source": "y_pred.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f3e3f48faa6f96f3ff77810880c893f475d802e5"
      },
      "cell_type": "code",
      "source": "results = pd.DataFrame({\"question_text\": test_df[\"question_text\"], \"prediction\": y_pred.flatten()})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b57e0c780228f8ded2e612ce4a2596046183d559"
      },
      "cell_type": "code",
      "source": "# Some legitimate questions\nresults.sort_values('prediction').head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "625ef5ad8fb4c00376f785b46ef621e95baf9eb8"
      },
      "cell_type": "code",
      "source": "results[results.prediction > .4].sort_values('prediction').head()['question_text'].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "596581857c08a6634231a166ca9d698845607380"
      },
      "cell_type": "code",
      "source": "results[results.prediction > .5].sort_values('prediction').head()['question_text'].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e27e42642d22042f27de1182851b82ff7de8a1f5"
      },
      "cell_type": "code",
      "source": "# Controversial, but sincere\nresults[results.prediction > .6].sort_values('prediction').head()['question_text'].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d4605d6a1c59d8f87389e0f2d8fef1d5e94b7fa3"
      },
      "cell_type": "code",
      "source": "# Questions are getting increasingly stupid...\nresults[results.prediction > .7].sort_values('prediction').head()['question_text'].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d853d47086d60a0ee8adb3f9a9db8c26422e78a6"
      },
      "cell_type": "code",
      "source": "# I'm losing IQ points just reading some of these...\nresults[results.prediction > .8].sort_values('prediction').sample(10)['question_text'].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b6d03abdbfefc5b425e34a9518510fafd0adf7d"
      },
      "cell_type": "code",
      "source": "# See the worst of humanity (you'd rather not -- commenting this out!)\n# results.sort_values('prediction', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "075e9c3286d98671e1c85821c47319d9721754db"
      },
      "cell_type": "code",
      "source": "y_te = (y_pred[:,0] > 0.5).astype(np.int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4374a7e043f07aea809d1039954732293a86a93"
      },
      "cell_type": "code",
      "source": "y_te",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b3282f69be6eea23ca2a277974d9fb75b5b13bf"
      },
      "cell_type": "code",
      "source": "submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0c9b0f0b6a70384da0f1385b88d34377011f47ab"
      },
      "cell_type": "markdown",
      "source": "# Word2vec"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "671a5ac8e06ac17646ab5c44d57a412805b6a70c"
      },
      "cell_type": "code",
      "source": "GLOVE_DIR = \"../input/embeddings/glove.840B.300d/\"\n\n# first, build index mapping words in the embeddings set\n# to their embedding vector\n\nprint('Indexing word vectors.')\n\nembeddings_index = {}\nwith open(os.path.join(GLOVE_DIR, 'glove.840B.300d.txt'), 'r', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = ''.join(values[:-300])\n        coefs = np.asarray(values[-300:], dtype='float32')\n        embeddings_index[word] = coefs\n\nprint('Found %s word vectors.' % len(embeddings_index))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4af0637f01c4efe110f74fc549f9e8217c33f965"
      },
      "cell_type": "code",
      "source": "print('Preparing embedding matrix.')\n\n### prepare embedding matrix ###\n\n# Use the max if there are more words than the max in our corpus\nnum_words = min(MAX_NB_WORDS, len(word_index))\nprint(num_words, 'words')\nembedding_matrix = np.zeros((num_words+1, EMBEDDING_DIM))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b7bee43ed13211929f637d56f4ece8eb125922e6"
      },
      "cell_type": "code",
      "source": "from keras.layers import (Embedding, Conv1D, MaxPooling1D,\n                          Flatten, Dropout, Activation)\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\n\nsgd = SGD(lr=0.0001, momentum=0.6, nesterov=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec6bc686d16d13649df33ac67a1ab6aa19d60bee"
      },
      "cell_type": "code",
      "source": "MAX_SEQUENCE_LENGTH = 100\nMAX_NB_WORDS = 50000\nEMBEDDING_DIM = 300\n\nword_index = tokenizer.word_index\n\nfor word, i in word_index.items():\n    if i >= MAX_NB_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\n# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(num_words+1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17f192c982c339233bc4fbed23d5131864c8a54a"
      },
      "cell_type": "code",
      "source": "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\n\nx = Conv1D(256, 5, activation='relu')(embedded_sequences)\nx = MaxPooling1D(3)(x)\n\nx = Conv1D(256, 5, activation='relu')(x)\nx = MaxPooling1D(3)(x)\n\nx = Conv1D(256, 5, activation='relu')(x)\nx = MaxPooling1D(3)(x)\n\nx = Flatten()(x)\nx = Dense(8, activation='relu')(x)\npreds = Dense(1, activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=sgd,\n              metrics=['acc'])\n\n# happy learning!\nmodel.fit(X_tra, y_tra, validation_data=(X_val, y_val),\n          epochs=10, batch_size=32, verbose=1, callbacks=[lr, es])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f18abe29a3257557556cb8a40e4f433cefef37e"
      },
      "cell_type": "code",
      "source": "filters = 250\nkernel_size = 3\nhidden_dims = 250\nembedding_dims = 100\n\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\nmodel.add(Dropout(0.2))\n\n# we add a Convolution1D, which will learn filters\n# word group filters of size filter_length:\nmodel.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1))\n# we use max pooling:\nmodel.add(GlobalMaxPooling1D())\n\n# We add a vanilla hidden layer:\nmodel.add(Dense(hidden_dims))\nmodel.add(Dropout(0.2))\nmodel.add(Activation('relu'))\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\n\n# happy learning!\nmodel.fit(X_tra, y_tra, validation_data=(X_val, y_val),\n          epochs=10, batch_size=32, verbose=1, callbacks=[lr, es])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3580ba3d7b1159be5340a31d9c2c85e608751157"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbf9b5713457707a0d90eda1b6cbbe99fff518d6"
      },
      "cell_type": "code",
      "source": "model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "561aafa92d82666f6a269274f4e85bcfb2b53d77"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}