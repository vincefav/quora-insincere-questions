{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "train_df = pd.read_csv(\"../input/train.csv\")\nX_train = train_df[\"question_text\"].fillna(\"zxc\").values\ntest_df = pd.read_csv(\"../input/test.csv\")\nX_test = test_df[\"question_text\"].fillna(\"zxc\").values\ny = train_df[\"target\"]\n\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c44c7dcccd256a5c88d793d9fa9599b3016072e8"
      },
      "cell_type": "code",
      "source": "# Wow, people can be awful :(\ntrain_df[train_df.target == 1].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e6f0fae8e57fca2bd3e90990d469257ca8327e2c"
      },
      "cell_type": "code",
      "source": "# Naive Bayes\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV\n\nkf = StratifiedKFold(n_splits=3)\n\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer()),\n    ('classifier', MultinomialNB()),\n])\n\nparam_grid = {\n    'tfidf__min_df': [20,],\n    'tfidf__max_df': [.4,],\n    'tfidf__ngram_range': [(1,2)],\n}\n\ngrid = GridSearchCV(pipeline, param_grid, cv=kf, scoring='accuracy').fit(X_train, y)\nclf = grid.best_estimator_\nprint(clf)\nprint()\nprint(cross_val_score(clf, X_train, y, cv=kf, scoring='accuracy').mean())\n\n# 0.949718325755045",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ddbf5d6b071d2e8f82bec498e68ab56983c18a04"
      },
      "cell_type": "code",
      "source": "words = {}\n\nfor word, num in clf.named_steps['tfidf'].vocabulary_.items():\n    prob = clf.predict_proba([word])[0][1]\n    words[word] = prob\n\ndf_topwords = pd.DataFrame([words]).transpose()\ndf_topwords.columns = ['probability']\n\ndf_topwords.sort_values('probability', ascending=False, inplace=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ee67698e60afd45a6529fb5d3f295e0e2f7dfe57"
      },
      "cell_type": "code",
      "source": "# Lots of racism and mentions of... castration?\n# People can be awful.\ndf_topwords",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "38a308e5661046f97af541061420bb29f0d5eadb"
      },
      "cell_type": "code",
      "source": "from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, concatenate\nfrom keras.layers import (CuDNNGRU, CuDNNLSTM, Bidirectional, GlobalAveragePooling1D,\n                          GlobalMaxPooling1D)\nfrom keras.preprocessing import text, sequence",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c2fe4020ebc3ddf16a3c2f1c53eb67d376f25561"
      },
      "cell_type": "code",
      "source": "maxlen = 100\nmax_features = 30000\n\n# Tokenize\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)\n\n# Sequence\nx_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test, maxlen=maxlen)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "805f9708fd42f212b238d254c6958b63b0bd4bb8"
      },
      "cell_type": "code",
      "source": "tokenizer.num_words",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d702fb1a3ae2b88745f178a2753c60f43ecd32ce"
      },
      "cell_type": "code",
      "source": "def get_model():\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, 100)(inp)\n    x = CuDNNLSTM(64, return_sequences=True)(x)\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    conc = concatenate([avg_pool, max_pool])\n    outp = Dense(1, activation=\"sigmoid\")(conc)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel = get_model()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "93f433ff59da870f5159fc06d825c68f37e63111"
      },
      "cell_type": "code",
      "source": "def get_model():\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, 100)(inp)\n    x = CuDNNLSTM(64)(x)\n    x = Dense(32, activation=\"relu\")(x)\n    outp = Dense(1, activation=\"sigmoid\")(x)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model\n\nmodel = get_model()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fec390becf54d8d33f71ca13d613696980a0776e"
      },
      "cell_type": "code",
      "source": "batch_size = 128\nepochs = 10",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6dc865296d4f0254fc04aefe07176d7917f3397"
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y, test_size = 0.1, random_state=42)\n\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nes = EarlyStopping(monitor='val_acc', min_delta=.003, patience=2)\nlr = ReduceLROnPlateau(monitor='val_acc', min_delta=.003, patience=1)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0b112231e50369d9e45a687d8e985be999adcb04"
      },
      "cell_type": "code",
      "source": "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n                  verbose=True, callbacks=[es, lr])\n\n\ny_pred = model.predict(x_test, batch_size=1024)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7cdc05e409d38dbef23e0f40786147b8b40fb3eb"
      },
      "cell_type": "code",
      "source": "# 50000 loss: 0.1162 - acc: 0.9545 - val_loss: 0.1052 - val_acc: 0.9582",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f1bcd395c3b5831252361217c742eed7cf40e02"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f3e3f48faa6f96f3ff77810880c893f475d802e5"
      },
      "cell_type": "code",
      "source": "results = pd.DataFrame({\"question_text\": test_df[\"question_text\"], \"prediction\": y_pred.flatten()})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c98ffb1be18555273fc5fc45c4afa224e120b4e4"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import roc_auc_score\n# y_pred_val = model.predict(X_val, batch_size=1024)\nroc_auc_score(y_val, y_pred_val.flatten())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b57e0c780228f8ded2e612ce4a2596046183d559"
      },
      "cell_type": "code",
      "source": "# Some legitimate questions\nresults.sort_values('prediction').head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "625ef5ad8fb4c00376f785b46ef621e95baf9eb8"
      },
      "cell_type": "code",
      "source": "results[results.prediction > .4].sort_values('prediction').head()['question_text'].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "596581857c08a6634231a166ca9d698845607380"
      },
      "cell_type": "code",
      "source": "results[results.prediction > .5].sort_values('prediction').head()['question_text'].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e27e42642d22042f27de1182851b82ff7de8a1f5"
      },
      "cell_type": "code",
      "source": "# Controversial, but sincere\nresults[results.prediction > .6].sort_values('prediction').head()['question_text'].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d4605d6a1c59d8f87389e0f2d8fef1d5e94b7fa3"
      },
      "cell_type": "code",
      "source": "# Questions are getting increasingly stupid...\nresults[results.prediction > .7].sort_values('prediction').head()['question_text'].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d853d47086d60a0ee8adb3f9a9db8c26422e78a6"
      },
      "cell_type": "code",
      "source": "# I'm losing IQ points just reading some of these...\nresults[results.prediction > .8].sort_values('prediction').sample(10)['question_text'].tolist()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b6d03abdbfefc5b425e34a9518510fafd0adf7d"
      },
      "cell_type": "code",
      "source": "# See the worst of humanity (you'd rather not -- commenting this out!)\n# results.sort_values('prediction', ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "075e9c3286d98671e1c85821c47319d9721754db"
      },
      "cell_type": "code",
      "source": "y_te = (y_pred[:,0] > 0.5).astype(np.int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e4374a7e043f07aea809d1039954732293a86a93"
      },
      "cell_type": "code",
      "source": "y_te",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2b3282f69be6eea23ca2a277974d9fb75b5b13bf"
      },
      "cell_type": "code",
      "source": "submit_df = pd.DataFrame({\"qid\": test_df[\"qid\"], \"prediction\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0c9b0f0b6a70384da0f1385b88d34377011f47ab"
      },
      "cell_type": "markdown",
      "source": "# Word2vec"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "671a5ac8e06ac17646ab5c44d57a412805b6a70c"
      },
      "cell_type": "code",
      "source": "GLOVE_DIR = \"../input/embeddings/glove.840B.300d/\"\n\n# first, build index mapping words in the embeddings set\n# to their embedding vector\n\nprint('Indexing word vectors.')\n\nembeddings_index = {}\nwith open(os.path.join(GLOVE_DIR, 'glove.840B.300d.txt'), 'r', encoding='utf-8') as f:\n    for line in f:\n        values = line.split()\n        word = ''.join(values[:-300])\n        coefs = np.asarray(values[-300:], dtype='float32')\n        embeddings_index[word] = coefs\n\nprint('Found %s word vectors.' % len(embeddings_index))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4af0637f01c4efe110f74fc549f9e8217c33f965"
      },
      "cell_type": "code",
      "source": "print('Preparing embedding matrix.')\n\n### prepare embedding matrix ###\n\n# Use the max if there are more words than the max in our corpus\nnum_words = min(MAX_NB_WORDS, len(word_index))\nprint(num_words, 'words')\nembedding_matrix = np.zeros((num_words+1, EMBEDDING_DIM))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b7bee43ed13211929f637d56f4ece8eb125922e6"
      },
      "cell_type": "code",
      "source": "from keras.layers import (Embedding, Conv1D, MaxPooling1D,\n                          Flatten, Dropout, Activation)\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\n\nsgd = SGD(lr=0.0001, momentum=0.6, nesterov=True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ec6bc686d16d13649df33ac67a1ab6aa19d60bee"
      },
      "cell_type": "code",
      "source": "MAX_SEQUENCE_LENGTH = 100\nMAX_NB_WORDS = 50000\nEMBEDDING_DIM = 300\n\nword_index = tokenizer.word_index\n\nfor word, i in word_index.items():\n    if i >= MAX_NB_WORDS:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector\n\n# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(num_words+1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17f192c982c339233bc4fbed23d5131864c8a54a"
      },
      "cell_type": "code",
      "source": "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\n\nx = Conv1D(256, 5, activation='relu')(embedded_sequences)\nx = MaxPooling1D(3)(x)\n\nx = Conv1D(256, 5, activation='relu')(x)\nx = MaxPooling1D(3)(x)\n\nx = Conv1D(256, 5, activation='relu')(x)\nx = MaxPooling1D(3)(x)\n\nx = Flatten()(x)\nx = Dense(8, activation='relu')(x)\npreds = Dense(1, activation='softmax')(x)\n\nmodel = Model(sequence_input, preds)\nmodel.compile(loss='binary_crossentropy',\n              optimizer=sgd,\n              metrics=['acc'])\n\n# happy learning!\nmodel.fit(X_tra, y_tra, validation_data=(X_val, y_val),\n          epochs=10, batch_size=32, verbose=1, callbacks=[lr, es])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f18abe29a3257557556cb8a40e4f433cefef37e"
      },
      "cell_type": "code",
      "source": "filters = 250\nkernel_size = 3\nhidden_dims = 250\nembedding_dims = 100\n\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\nmodel.add(Dropout(0.2))\n\n# we add a Convolution1D, which will learn filters\n# word group filters of size filter_length:\nmodel.add(Conv1D(filters,\n                 kernel_size,\n                 padding='valid',\n                 activation='relu',\n                 strides=1))\n# we use max pooling:\nmodel.add(GlobalMaxPooling1D())\n\n# We add a vanilla hidden layer:\nmodel.add(Dense(hidden_dims))\nmodel.add(Dropout(0.2))\nmodel.add(Activation('relu'))\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(1))\nmodel.add(Activation('sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\n\n# happy learning!\nmodel.fit(X_tra, y_tra, validation_data=(X_val, y_val),\n          epochs=10, batch_size=32, verbose=1, callbacks=[lr, es])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3580ba3d7b1159be5340a31d9c2c85e608751157"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fbf9b5713457707a0d90eda1b6cbbe99fff518d6"
      },
      "cell_type": "code",
      "source": "model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "561aafa92d82666f6a269274f4e85bcfb2b53d77"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}